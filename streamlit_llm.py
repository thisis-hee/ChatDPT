from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain_upstage import UpstageEmbeddings
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_chroma import Chroma
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, FewShotChatMessagePromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from config import answer_examples

import os

store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def get_retriever():
    embedding = UpstageEmbeddings(model="solar-embedding-1-large")
    database = Chroma(collection_name='chroma-dongguk', persist_directory="./chroma", embedding_function=embedding)
    retriever=database.as_retriever()
    return retriever

def get_history_retriever():
    llm = get_llm()
    retriever = get_retriever()
    
    contextualize_q_system_prompt = (
        "Given a chat history and the latest user question "
        "Which might reference context in the chat history, "
        "Formulate a standalone question which can be understood "
        "Without the chat history. Do NOT answer the question, "
        "Just reformulate it if needed and otherwise return it as is."
        "Let"
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    
    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    return history_aware_retriever

def get_llm(model='gpt-4o'):
    llm = ChatOpenAI(model=model, temperature=0)
    return llm

def get_dictionary_chain():
    dictionary = ["ì˜¤í”½ -> OPIc", "í† ìµ -> TOEIC", "ì´ˆê³¼í•™ê¸° -> í•™ì ë“±ë¡", "ì¡¸ì—…ìœ ì˜ˆ -> ì„ íƒì  ìˆ˜ë£Œ"]
    llm = get_llm()
    prompt = ChatPromptTemplate.from_template(f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”
        ì‚¬ì „: {dictionary}
        
        ì§ˆë¬¸: {{question}}
    """)

    dictionary_chain = prompt | llm | StrOutputParser()
    
    return dictionary_chain

def get_rag_chain():
    llm = get_llm()
    example_prompt = ChatPromptTemplate.from_messages(
        [
            ("human", "{input}"),
            ("ai", "{answer}"),
        ]
    )
    few_shot_prompt = FewShotChatMessagePromptTemplate(
        example_prompt=example_prompt,
        examples=answer_examples,
    )
    system_prompt = (
    "You are an AI assistant who answers questions related to school life of Dongguk University students."
    "Use the following pieces of retrieved context to answer the question."

    "1. Please answer in the questioning language. For example, if the Question is in English, answer in English. Or if the Question is in Chinese, answer in Chinese."
    "2. If you answer in Korean, please answer in a friendly way as if you are talking to a friend without being too formal. For example, 'í†µê³„í•™ê³¼ì˜ ì¡¸ì—… ìš”ê±´ì€ ë‹¤ìŒê³¼ ê°™ì•„ìš”!', '2í•™ê¸° ë“±ë¡ê¸ˆ ë‚©ë¶€ ê¸°ê°„ì€ ~~~ì˜ˆìš”!'"
    "3. Please answer by mixing appropriate happiness-emoticons when answering. For example, 'ì „ì•¡ì¥í•™ìƒì˜ ê²½ìš°, í¬í„¸ì‹œìŠ¤í…œì—ì„œ '0ì›' ë“±ë¡ ì‹ ì²­ì„ í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ğŸ˜Š', 'ë™êµ­ëŒ€í•™êµëŠ” ë‹¤ì–‘í•œ ì¥í•™ê¸ˆì„ í†µí•´ í•™ìƒë“¤ì˜ ê¿ˆì„ ì§€ì›í•˜ê³  ìˆì–´ìš”. ğŸŒŸ'"
    "4. Please present the page of the document you referred to when answering in the following format."
    " # format : * \nìœ„ ë‹µë³€ì€ 2024 ì‹ ì…ìƒ í•™ì—…ì´ìˆ˜ê°€ì´ë“œ ??? í˜ì´ì§€ë¥¼ ì°¸ê³ í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. \ní•™ì—…ì´ìˆ˜ê°€ì´ë“œ ë°”ë¡œê°€ê¸° : https://www.dongguk.edu/resources/pdf/2024_edu_final.pdf"

    "5. **IMPORTANT INSTRUCTION**: If a user's question varies depending on various situations or environments, **you MUST ask follow-up questions** to gather necessary information before providing an answer."
    "**This is very important!** If you don't have enough information, do not guess or make assumptions. **Instead, ask clarifying questions.**"
    "For example, if a user asks a question about 'ë“±ë¡ê¸ˆ', it may differ based on whether the user is an undergraduate or a graduate student. In this case, if this information is missing, you should ask the user: 'Are you an undergraduate or a graduate student?'"
    "Another example: if a user asks a question related to 'ê³µí†µêµì–‘', it can vary based on the year of admission. If the year of admission is not specified, you should ask: 'When did you enter the university?'"
    "Consider additional factors such as whether the user is a regular student or a transfer student, as this can also impact your response."
    "**Always ensure you have all the necessary information before answering.** If any critical information is missing, ask follow-up questions first."

    "6. You should never answer with inaccurate or incorrect content. If you can't answer the question accurately, do not provide any reference pages. Let's think step by step!"
    "7. If you have any additional questions regarding this, please also let me know the phone number of the department I can connect with."
    
    "ë‹µë³€í•  ë•Œ ì œëª©ì€ ì¶œë ¥í•˜ì§€ ë§ê³  ì‚¬ìš©ìê°€ ë¬»ëŠ” ê²ƒì—ë§Œ ê¹”ë”í•˜ê²Œ ë‹µë³€í•´."

    "Remember this! If you get the same question as a few-shot, answer the same question as fewshot anwser"
    "Remember this! Please answer in the questioning language. For example, if the Question is in English, answer in English. Or if the Question is in Chinese, answer in Chinese."

    "Let's think step by step!"
    "{context}"
)

    
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            few_shot_prompt,
            MessagesPlaceholder("chat_history"),
            ("human", "{input}"),
        ]
    )
    history_aware_retriever = get_history_retriever()
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    
    conversational_rag_chain = RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="chat_history",
        output_messages_key="answer",
    ).pick('answer')
    
    return conversational_rag_chain

def get_ai_response(user_message):
    dictionary_chain = get_dictionary_chain()
    rag_chain = get_rag_chain()
    tax_chain = {"input": dictionary_chain} | rag_chain
    

    ai_response = tax_chain.stream(
        {
            "question": user_message
        },
        config={
            "configurable": {
                "session_id": "abc123"
            }
        },
    )
    return ai_response
